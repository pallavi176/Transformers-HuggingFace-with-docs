{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Transformer-Introduction HuggingFace ecosystem Transformers: https://github.com/huggingface/transformers Datasets: https://github.com/huggingface/datasets Tokenizers: https://github.com/huggingface/tokenizers Accelerate: https://github.com/huggingface/accelerate HuggingFace Hub: https://huggingface.co/models Common NLP tasks Classifying whole sentences: sentiment analysis, spam classification, verify sentence grammer, logical relation between sentences Classifying each word in a sentence: NER, pos tag Generating text content: Text generation, fill in the blanks with masked words Extracting an answer from a text: QnA Generating a new sentence from an input text: NMT, text summarization","title":"Introduction"},{"location":"#transformer-introduction","text":"HuggingFace ecosystem Transformers: https://github.com/huggingface/transformers Datasets: https://github.com/huggingface/datasets Tokenizers: https://github.com/huggingface/tokenizers Accelerate: https://github.com/huggingface/accelerate HuggingFace Hub: https://huggingface.co/models Common NLP tasks Classifying whole sentences: sentiment analysis, spam classification, verify sentence grammer, logical relation between sentences Classifying each word in a sentence: NER, pos tag Generating text content: Text generation, fill in the blanks with masked words Extracting an answer from a text: QnA Generating a new sentence from an input text: NMT, text summarization","title":"Transformer-Introduction"},{"location":"1tranformer-pipeline/","text":"Pipeline Function The pipeline function returns an end-to-end object that performs an NLP task on one or several texts. It includes steps: Pre-Processing Model Post-Processing from transformers import pipeline classifier = pipeline ( \"sentiment-analysis\" ) classifier ( \"I've been waiting for a HuggingFace course my whole life.\" ) By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again. Available pipelines are: feature-extraction (get the vector representation of a text) fill-mask: to fill in the blanks ner (named entity recognition): identifies persons, locations, or organizations question-answering: answers questions using information from a given context sentiment-analysis summarization: reducing a text into a shorter text while keeping almost of the important aspects referenced in the text text-generation translation zero-shot-classification: classify on the basis of listed classes/labels. Transformer Models: June 2018: GPT October 2018: BERT February 2019: GPT-2 October 2019: DistilBERT , a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT\u2019s performance. October 2019: BART and T5 : Both Encoder and Decoder May 2020: GPT-3 : an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning) Transformer Model Categories: GPT-like (also called auto-regressive Transformer models) BERT-like (also called auto-encoding Transformer models) BART/T5-like (also called sequence-to-sequence Transformer models) Transformers are: Language models: trained on large amounts of raw text in a self-supervised fashion Self-supervised learner: here objective is automatically computed from the inputs of the model pretrained model goes through transfer learning, ie, fine-tuned in supervised way using human-annotated labels on given task. Casual Language Modeling: output depends on the past and present inputs, but not the future ones Masked language modeling: predicts a masked word in the sentence Transfer Learning The act of initializing a model with another model's weights. Training from scratch requires more data and more compute to achieve comparable results. In NLP, predicting the next word is a common pretraining objective.(GPT) Another common pretraining objective in text is to gues the value of randomly masked words.(BERT) Usually, Transfer Learning is applied by dropping the head of the pretrained model while keeping its body. The pretrained model helps by transferring its knowledge but it also transfers the bias it may contain. OpenAI studied the bias predictions of its GPT-3 model. pre-training fine-tuning training from scratch transfer learning Architectures vs. checkpoints Architecture: This is the skeleton of the model \u2014 the definition of each layer and each operation that happens within the model. Checkpoints: These are the weights that will be loaded in a given architecture. Model: This is an umbrella term that isn\u2019t as precise as \u201carchitecture\u201d or \u201ccheckpoint\u201d: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity. Bert Family: ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa Decoder family: CTRL, GPT, GPT-2, Transformer XL, GPT Neo Encoder-decoder(sequence-to-sequence) models: BART, mBART, Marian, T5, Pegasus, ProphetNet, M2M100","title":"Pipeline"},{"location":"1tranformer-pipeline/#bert-family","text":"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa","title":"Bert Family:"},{"location":"1tranformer-pipeline/#decoder-family","text":"CTRL, GPT, GPT-2, Transformer XL, GPT Neo","title":"Decoder family:"},{"location":"1tranformer-pipeline/#encoder-decodersequence-to-sequence-models","text":"BART, mBART, Marian, T5, Pegasus, ProphetNet, M2M100","title":"Encoder-decoder(sequence-to-sequence) models:"},{"location":"2UsingTransformerIntro/","text":"Introduction In this section we will: Learn the basic building blocks of a Transformer model. Learn what makes up a tokenization pipeline. See how to use a Transformer model in practice. Learn how to leverage a tokenizer to convert text to tensors that are understandable by the model. Set up a tokenizer and a model together to get from text to predictions. Learn the limitations of input IDs, and learned about attention masks. Play around with versatile and configurable tokenizer methods.","title":"Introduction"},{"location":"2UsingTransformerIntro/#introduction","text":"In this section we will: Learn the basic building blocks of a Transformer model. Learn what makes up a tokenization pipeline. See how to use a Transformer model in practice. Learn how to leverage a tokenizer to convert text to tensors that are understandable by the model. Set up a tokenizer and a model together to get from text to predictions. Learn the limitations of input IDs, and learned about attention masks. Play around with versatile and configurable tokenizer methods.","title":"Introduction"},{"location":"2_1BehindPipeline/","text":"Behind the pipeline Preprocessing with a tokenizer -> All preprocessing needs to be done in exactly the same way as when the model was pretrained. -> To do this, we use the AutoTokenizer class and its from_pretrained() method. -> Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it. from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) -> Next step is to convert the list of input IDs to tensors. -> To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument: raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this so much!\" , ] inputs = tokenizer ( raw_inputs , padding = True , truncation = True , return_tensors = \"tf\" ) #return_tensors: To specify the type of tensors we want to get back # truncation=True, Any sentence longer than the maximum the model can handle is truncated print ( inputs ) -> The output itself is a dictionary containing two keys, input_ids and attention_mask. -> input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. -> attention mask indicates where padding has been applied, so the model does not pay attention to it. Going through the model -> Pretrained model can be downloaded similarly as tokenizer using TFAutoModel class. -> TFAutoModel class loads a model without its pretraining head. from transformers import TFAutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = TFAutoModel . from_pretrained ( checkpoint ) # downloads configuration of the model as well as pre-trained weights, only initantiates the body of the model # downloaded same checkpoint used in pipeline(cached already) and instantiated a model with it -> This architecture contains only the base Transformer module: given some inputs, it outputs what we\u2019ll call hidden states, also known as features. For each model input, we\u2019ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model. -> While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the head. Different NLP tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it. A high-dimensional vector? The vector output by the Transformer module is usually large. It generally has three dimensions: Batch size: The number of sequences processed at a time (2 in our example). Sequence length: The length of the numerical representation of the sequence (16 in our example). Hidden size: The vector dimension of each model input. It is said to be \u201chigh dimensional\u201d because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more). outputs = model ( inputs ) print ( outputs . last_hidden_state . shape ) Note that the outputs of Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]). Model heads: Making sense out of numbers The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. The output of the Transformer model is sent directly to the model head to be processed. There are many different architectures available in Transformers, with each one designed around tackling a specific task: *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification, etc For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the TFAutoModel class, but TFAutoModelForSequenceClassification: from transformers import TFAutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( inputs ) print ( outputs . logits . shape ) - Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label): - Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2. Postprocessing the output print ( outputs . logits ) logits are raw, unnormalized scores outputted by the last layer of the model. - To be converted to probabilities, they need to go through a SoftMax layer (all Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy) import tensorflow as tf predictions = tf . math . softmax ( outputs . logits , axis =- 1 ) print ( predictions ) It returns probability scores. To get the labels corresponding to each position, we can inspect the id2label attribute of the model config. model . config . id2label {0: 'NEGATIVE', 1: 'POSITIVE'}","title":"Behind The Pipeline"},{"location":"2_1BehindPipeline/#behind-the-pipeline","text":"","title":"Behind the pipeline"},{"location":"2_1BehindPipeline/#preprocessing-with-a-tokenizer","text":"-> All preprocessing needs to be done in exactly the same way as when the model was pretrained. -> To do this, we use the AutoTokenizer class and its from_pretrained() method. -> Using the checkpoint name of our model, it will automatically fetch the data associated with the model\u2019s tokenizer and cache it. from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) -> Next step is to convert the list of input IDs to tensors. -> To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument: raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this so much!\" , ] inputs = tokenizer ( raw_inputs , padding = True , truncation = True , return_tensors = \"tf\" ) #return_tensors: To specify the type of tensors we want to get back # truncation=True, Any sentence longer than the maximum the model can handle is truncated print ( inputs ) -> The output itself is a dictionary containing two keys, input_ids and attention_mask. -> input_ids contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. -> attention mask indicates where padding has been applied, so the model does not pay attention to it.","title":"Preprocessing with a tokenizer"},{"location":"2_1BehindPipeline/#going-through-the-model","text":"-> Pretrained model can be downloaded similarly as tokenizer using TFAutoModel class. -> TFAutoModel class loads a model without its pretraining head. from transformers import TFAutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = TFAutoModel . from_pretrained ( checkpoint ) # downloads configuration of the model as well as pre-trained weights, only initantiates the body of the model # downloaded same checkpoint used in pipeline(cached already) and instantiated a model with it -> This architecture contains only the base Transformer module: given some inputs, it outputs what we\u2019ll call hidden states, also known as features. For each model input, we\u2019ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model. -> While these hidden states can be useful on their own, they\u2019re usually inputs to another part of the model, known as the head. Different NLP tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it.","title":"Going through the model"},{"location":"2_1BehindPipeline/#a-high-dimensional-vector","text":"The vector output by the Transformer module is usually large. It generally has three dimensions: Batch size: The number of sequences processed at a time (2 in our example). Sequence length: The length of the numerical representation of the sequence (16 in our example). Hidden size: The vector dimension of each model input. It is said to be \u201chigh dimensional\u201d because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more). outputs = model ( inputs ) print ( outputs . last_hidden_state . shape ) Note that the outputs of Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0]).","title":"A high-dimensional vector?"},{"location":"2_1BehindPipeline/#model-heads-making-sense-out-of-numbers","text":"The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. The output of the Transformer model is sent directly to the model head to be processed. There are many different architectures available in Transformers, with each one designed around tackling a specific task: *Model (retrieve the hidden states) *ForCausalLM *ForMaskedLM *ForMultipleChoice *ForQuestionAnswering *ForSequenceClassification *ForTokenClassification, etc For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the TFAutoModel class, but TFAutoModelForSequenceClassification: from transformers import TFAutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( inputs ) print ( outputs . logits . shape ) - Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label): - Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.","title":"Model heads: Making sense out of numbers"},{"location":"2_1BehindPipeline/#postprocessing-the-output","text":"print ( outputs . logits ) logits are raw, unnormalized scores outputted by the last layer of the model. - To be converted to probabilities, they need to go through a SoftMax layer (all Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy) import tensorflow as tf predictions = tf . math . softmax ( outputs . logits , axis =- 1 ) print ( predictions ) It returns probability scores. To get the labels corresponding to each position, we can inspect the id2label attribute of the model config. model . config . id2label {0: 'NEGATIVE', 1: 'POSITIVE'}","title":"Postprocessing the output"},{"location":"2_2Models/","text":"Instantiate a Transformers model TFAutoModel API allows you to instantiate a pretrained model from any checkpoint. from transformers import TFAutoModel bert_model = TFAutoModel . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_model )) gpt_model = TFAutoModel . from_pretrained ( \"gpt2\" ) print ( type ( gpt_model )) bart_model = TFAutoModel . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_model )) Checkpoint or local folder Behind the AutoModel.from_pretrained() method: config.json file: attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what Transformers version you were using when you last saved the checkpoint. tf_model.h5 file: known as the state dictionary; it contains all your model\u2019s weights configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters. The weights can be downloaded and cached (so future calls to the from_pretrained() method won\u2019t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable. To instantiate a pre-trained model, the AutoConfig API will first check in config file to look at the config class that should be used. The config class depends on the type of model(bert, gpt-2, etc). Once it attaches a proper config class, it can instantiate that configuration which is a blueprint to know how to create a model. It uses this configuration class to find the proper model which is combined to the logit configuration to load the model. this model is not yet trained model as it just being initialized with the random weights. The last step is to loads weight from the model file inside this model(above loaded model). The AutoConfig API allows you to instantiate the configuration of a pretrained model from any checkpoint: from transformers import AutoConfig bert_config = AutoConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) gpt_config = AutoConfig . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) bart_config = AutoConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) But you can aslo use the specific class if you know it: from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) from transformers import GPT2Config gpt_config = GPT2Config . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) from transformers import BartConfig bart_config = BartConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) The configuration(bert_config, gpt_config, bart_config) contains all the information needed to load the model/create the model architecture. from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( bert_config ) Then you can instantiate a given model with random weights from this config. ie, once we have the configuration we can create a model which has same architecture as the checkpoint which is from it was initialized. We can train it from scratch. We can also change any part of its configurations using keyword arguments # Same architecture as bert-base-cased from transformers import BertConfig , TFBertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = TFBertModel ( bert_config ) # Using only 10 layers instead of 12 from transformers import BertConfig , TFBertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" , num_hidden_layers = 10 ) bert_model = TFBertModel ( bert_config ) Saving a model To save a model, we just have to use the the save_pretrained method. from transformers import BertConfig , TFBertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = TFBertModel ( bert_config ) # Training code bert_model . save_pretrained ( \"my-bert-model\" ) Reloading a saved model from transformers import TFBertModel bert_model = TFBertModel . from_pretrained ( \"my-bert-model\" )","title":"Models"},{"location":"2_2Models/#instantiate-a-transformers-model","text":"TFAutoModel API allows you to instantiate a pretrained model from any checkpoint. from transformers import TFAutoModel bert_model = TFAutoModel . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_model )) gpt_model = TFAutoModel . from_pretrained ( \"gpt2\" ) print ( type ( gpt_model )) bart_model = TFAutoModel . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_model ))","title":"Instantiate a Transformers model"},{"location":"2_2Models/#checkpoint-or-local-folder","text":"Behind the AutoModel.from_pretrained() method: config.json file: attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what Transformers version you were using when you last saved the checkpoint. tf_model.h5 file: known as the state dictionary; it contains all your model\u2019s weights configuration is necessary to know your model\u2019s architecture, while the model weights are your model\u2019s parameters. The weights can be downloaded and cached (so future calls to the from_pretrained() method won\u2019t re-download them) in the cache folder, which defaults to ~/.cache/huggingface/transformers. You can customize your cache folder by setting the HF_HOME environment variable. To instantiate a pre-trained model, the AutoConfig API will first check in config file to look at the config class that should be used. The config class depends on the type of model(bert, gpt-2, etc). Once it attaches a proper config class, it can instantiate that configuration which is a blueprint to know how to create a model. It uses this configuration class to find the proper model which is combined to the logit configuration to load the model. this model is not yet trained model as it just being initialized with the random weights. The last step is to loads weight from the model file inside this model(above loaded model). The AutoConfig API allows you to instantiate the configuration of a pretrained model from any checkpoint: from transformers import AutoConfig bert_config = AutoConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) gpt_config = AutoConfig . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) bart_config = AutoConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) But you can aslo use the specific class if you know it: from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( type ( bert_config )) from transformers import GPT2Config gpt_config = GPT2Config . from_pretrained ( \"gpt2\" ) print ( type ( gpt_config )) from transformers import BartConfig bart_config = BartConfig . from_pretrained ( \"facebook/bart-base\" ) print ( type ( bart_config )) The configuration(bert_config, gpt_config, bart_config) contains all the information needed to load the model/create the model architecture. from transformers import BertConfig bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) print ( bert_config ) Then you can instantiate a given model with random weights from this config. ie, once we have the configuration we can create a model which has same architecture as the checkpoint which is from it was initialized. We can train it from scratch. We can also change any part of its configurations using keyword arguments # Same architecture as bert-base-cased from transformers import BertConfig , TFBertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = TFBertModel ( bert_config ) # Using only 10 layers instead of 12 from transformers import BertConfig , TFBertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" , num_hidden_layers = 10 ) bert_model = TFBertModel ( bert_config )","title":"Checkpoint or local folder"},{"location":"2_2Models/#saving-a-model","text":"To save a model, we just have to use the the save_pretrained method. from transformers import BertConfig , TFBertModel bert_config = BertConfig . from_pretrained ( \"bert-base-cased\" ) bert_model = TFBertModel ( bert_config ) # Training code bert_model . save_pretrained ( \"my-bert-model\" )","title":"Saving a model"},{"location":"2_2Models/#reloading-a-saved-model","text":"from transformers import TFBertModel bert_model = TFBertModel . from_pretrained ( \"my-bert-model\" )","title":"Reloading a saved model"},{"location":"2_3Tokenizers/","text":"Tokenizers: In NLP, most of the data we handle is raw text. Tokenizers is used to transform raw text to numbers. Tokenizer's objective is to find a meaningful representation. 3 distict tokenizations: Word-based Character-based Subword-based Word-based The text is split on spaces Other rules, such as punctuation, may be added In this algorithm, each word has a specific ID/number attributed to it Limits: very similar words have entirely different meanings. eg: dog vs dogs the vocabulary can end up very large due to lot of different words large vocabularies result in heavy models loss of meaning across very similar words large quantity of out-of-vocabulary tokens We can limit the amount of words we add to the vocabulary by ignoring certain words which are not necessary Out of vocabulary words result in a loss of information, since model will have the exact same representation for all words that it doesn't know, [UNK], unknown. tokenized_text = \"Jim Henson was a puppeteer\" . split () print ( tokenized_text ) # ['Jim', 'Henson', 'was', 'a', 'puppeteer'] Character-based Splitting a raw text into characters Character-based vocabularies are slimmer There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. Limits: intuitively, it\u2019s less meaningful: each character doesn\u2019t mean a lot on its own in compared to any word Their sequences are translated into very large amounts of tokens to be processed by the model and this can have an impact on the size of the contants the model will carry around and it will reduce the size of the text we can use as input for a model which is often limited very long sequences less meaningful individual tokens Subword tokenization Splitting a raw text into subwords Finding a middle ground between word and character-based algorithms Subword-based tokenization lies between character and worf-based algorithm Frequently used words should not be split into smaller subwords Rare words should be decomposed into meaningful subwords Subwords help identify similar syntactic or semantic situations in text Subword tokenization algorithms can identify start of word tokens and which tokens complete start of words Most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm. WordPiece: BERT, DistilBERT Unigram: XLNet, ALBERT Byte-Pair Encoding: GPT-2, RoBERTa These approaches help in reducing the vocabulary sizes by sharing information across different words having the ability to have prefixes and suffixes understood as such. They keep meaning across very similar words by recognizing similar tokens making them up. Tokenizer Pipeline A tokenizer takes texts as inputs and outputs numbers the associated model can make sense of from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] Tokenization pipeline: from input text to a list of numbers: Raw text: Let's try to tokenize! Tokens: [let,',s, try, to, token, ##ize,!] Special tokens: [[CLS],let,',s, try, to, token, ##ize,!,[SEP]] Input IDs: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] The first step of the pipeline is to split the text into tokens. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[let, ', s, try, to, token, ##ize, !] Albert tokenize will add a long underscore in front of all the tokens that added space before them which is a convention shared by all sentence-based tokenizers from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"albert-base-v1\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[_let, ', s, _try, _to, _to, ken, ize, !] The second step of the tokenization pipeline is to map those tokens to their respective ids as defined by the vocabulary of the tokenizer. This is why we need to download the file when we instantiate a tokenizer with from_pretrained() method. We have to make sure we use the same mapping as when the model was pretrained. To do this, we use convert_tokens_to_ids() method: from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( input_ids ) #[2292, 1005, 1055, 3046, 2000, 19204, 4697, 999] Lastly, the tokenizer adds special tokens the model expects by prepare_for_model() method: final_inputs = tokenizer . prepare_for_model ( input_ids ) print ( final_inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] Decoding The decode method allows us to check how the final output of the tokenizer translates back into text. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"[CLS] let's try to tokenize! [SEP]\" Roberta tokenizers uses special tokens as : & from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"roberta\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"<s> let's try to tokenize! </s>\" Hence, a tokenizer takes texts as inputs and outputs numbers the associated model can make sense of. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) # {'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}","title":"Tokenizers"},{"location":"2_3Tokenizers/#tokenizers","text":"In NLP, most of the data we handle is raw text. Tokenizers is used to transform raw text to numbers. Tokenizer's objective is to find a meaningful representation. 3 distict tokenizations: Word-based Character-based Subword-based","title":"Tokenizers:"},{"location":"2_3Tokenizers/#word-based","text":"The text is split on spaces Other rules, such as punctuation, may be added In this algorithm, each word has a specific ID/number attributed to it Limits: very similar words have entirely different meanings. eg: dog vs dogs the vocabulary can end up very large due to lot of different words large vocabularies result in heavy models loss of meaning across very similar words large quantity of out-of-vocabulary tokens We can limit the amount of words we add to the vocabulary by ignoring certain words which are not necessary Out of vocabulary words result in a loss of information, since model will have the exact same representation for all words that it doesn't know, [UNK], unknown. tokenized_text = \"Jim Henson was a puppeteer\" . split () print ( tokenized_text ) # ['Jim', 'Henson', 'was', 'a', 'puppeteer']","title":"Word-based"},{"location":"2_3Tokenizers/#character-based","text":"Splitting a raw text into characters Character-based vocabularies are slimmer There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. Limits: intuitively, it\u2019s less meaningful: each character doesn\u2019t mean a lot on its own in compared to any word Their sequences are translated into very large amounts of tokens to be processed by the model and this can have an impact on the size of the contants the model will carry around and it will reduce the size of the text we can use as input for a model which is often limited very long sequences less meaningful individual tokens","title":"Character-based"},{"location":"2_3Tokenizers/#subword-tokenization","text":"Splitting a raw text into subwords Finding a middle ground between word and character-based algorithms Subword-based tokenization lies between character and worf-based algorithm Frequently used words should not be split into smaller subwords Rare words should be decomposed into meaningful subwords Subwords help identify similar syntactic or semantic situations in text Subword tokenization algorithms can identify start of word tokens and which tokens complete start of words Most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm. WordPiece: BERT, DistilBERT Unigram: XLNet, ALBERT Byte-Pair Encoding: GPT-2, RoBERTa These approaches help in reducing the vocabulary sizes by sharing information across different words having the ability to have prefixes and suffixes understood as such. They keep meaning across very similar words by recognizing similar tokens making them up.","title":"Subword tokenization"},{"location":"2_3Tokenizers/#tokenizer-pipeline","text":"A tokenizer takes texts as inputs and outputs numbers the associated model can make sense of from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] Tokenization pipeline: from input text to a list of numbers: Raw text: Let's try to tokenize! Tokens: [let,',s, try, to, token, ##ize,!] Special tokens: [[CLS],let,',s, try, to, token, ##ize,!,[SEP]] Input IDs: [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102] The first step of the pipeline is to split the text into tokens. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[let, ', s, try, to, token, ##ize, !] Albert tokenize will add a long underscore in front of all the tokens that added space before them which is a convention shared by all sentence-based tokenizers from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"albert-base-v1\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) print ( tokens ) #[_let, ', s, _try, _to, _to, ken, ize, !] The second step of the tokenization pipeline is to map those tokens to their respective ids as defined by the vocabulary of the tokenizer. This is why we need to download the file when we instantiate a tokenizer with from_pretrained() method. We have to make sure we use the same mapping as when the model was pretrained. To do this, we use convert_tokens_to_ids() method: from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" tokens = tokenizer . tokenize ( sequence ) input_ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( input_ids ) #[2292, 1005, 1055, 3046, 2000, 19204, 4697, 999] Lastly, the tokenizer adds special tokens the model expects by prepare_for_model() method: final_inputs = tokenizer . prepare_for_model ( input_ids ) print ( final_inputs ) #[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102]","title":"Tokenizer Pipeline"},{"location":"2_3Tokenizers/#decoding","text":"The decode method allows us to check how the final output of the tokenizer translates back into text. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"[CLS] let's try to tokenize! [SEP]\" Roberta tokenizers uses special tokens as : & from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"roberta\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) decoded_string = tokenizer . decode ( inputs [ \"input_ids\" ]) print ( decoded_string ) # \"<s> let's try to tokenize! </s>\" Hence, a tokenizer takes texts as inputs and outputs numbers the associated model can make sense of. from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"bert-base-cased\" ) sequence = \"Let's try to tokenize!\" inputs = tokenizer ( sequence ) print ( inputs ) # {'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 999, 102], # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}","title":"Decoding"},{"location":"2_4HandlingMultipleSequences/","text":"Handling multiple sequences Batching inputs together: Sentences we want to group inside a batch will often have different lengths from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] tokens = [ tokenizer . tokenize ( sentence ) for sentence in sentences ] ids = [ tokenizer . convert_tokens_to_ids ( token ) for token in tokens ] print ( ids [ 0 ]) print ( ids [ 1 ]) #[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] #[1045, 5223, 2023, 1012] You can't build a tensor with lists of different lengths import tensorflow as tf ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 ]] input_ids = tf . convert_to_tensor ( ids ) # Error: not a rectangualr shape Generally, we only truncate sentences when they are longer than the maximum length the model can handle Which is why we usually pad the smaller sentences to the length of the longest one! import tensorflow as tf ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] input_ids = tf . convert_to_tensor ( ids ) input_ids from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer . pad_token_id # Applying padding here Now that we have padded our sentences we can make a batch with them But just passing this through a transformers model will not give the right results. from transformers import TFAutoModelForSequenceClassification ids1 = tf . convert_to_tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ]] ) ids2 = tf . convert_to_tensor ([[ 1045 , 5223 , 2023 , 1012 ]]) all_ids = tf . convert_to_tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) print ( model ( ids1 ) . logits ) print ( model ( ids2 ) . logits ) print ( model ( all_ids ) . logits ) \"\"\" tensor([[-2.7276, 2.8789]], grad_fn=<AddmmBackward>) tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>) tensor([[-2.7276, 2.8789], [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>) \"\"\" This is because the attention layers use the padding tokens in the context they look at for each token in the sentence. Attention layers attend just the 4 tokens: [I, hate, this, !] Attention layers attend the 4 tokens and all padding tokens: [I, hate, this, !, [PAD], [PAD], [PAD], [PAD]] To tell the attention layers to ignore the padding tokens, we need to pass them an attention mask. all_ids = tf . convert_to_tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) # adding attention by creating attention mask attention_mask = tf . convert_to_tensor ( [[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) Here, attention layers will ignore the tokens marked with 0 in the attention mask. With the proper attention mask, predictions are the same for a given sentence, with or without padding. model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) output1 = model ( ids1 ) output2 = model ( ids2 ) print ( output1 . logits ) print ( output2 . logits ) output = model ( all_ids , attention_mask = attention_mask ) print ( output . logits ) Using with padding=True, the tokenizer can directly prepare the inputs with padding and the proper attention mask: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] print ( tokenizer ( sentences , padding = True ))","title":"Handling Multiple Sequences"},{"location":"2_4HandlingMultipleSequences/#handling-multiple-sequences","text":"","title":"Handling multiple sequences"},{"location":"2_4HandlingMultipleSequences/#batching-inputs-together","text":"Sentences we want to group inside a batch will often have different lengths from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] tokens = [ tokenizer . tokenize ( sentence ) for sentence in sentences ] ids = [ tokenizer . convert_tokens_to_ids ( token ) for token in tokens ] print ( ids [ 0 ]) print ( ids [ 1 ]) #[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] #[1045, 5223, 2023, 1012] You can't build a tensor with lists of different lengths import tensorflow as tf ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 ]] input_ids = tf . convert_to_tensor ( ids ) # Error: not a rectangualr shape Generally, we only truncate sentences when they are longer than the maximum length the model can handle Which is why we usually pad the smaller sentences to the length of the longest one! import tensorflow as tf ids = [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] input_ids = tf . convert_to_tensor ( ids ) input_ids from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer . pad_token_id # Applying padding here Now that we have padded our sentences we can make a batch with them But just passing this through a transformers model will not give the right results. from transformers import TFAutoModelForSequenceClassification ids1 = tf . convert_to_tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ]] ) ids2 = tf . convert_to_tensor ([[ 1045 , 5223 , 2023 , 1012 ]]) all_ids = tf . convert_to_tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) print ( model ( ids1 ) . logits ) print ( model ( ids2 ) . logits ) print ( model ( all_ids ) . logits ) \"\"\" tensor([[-2.7276, 2.8789]], grad_fn=<AddmmBackward>) tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>) tensor([[-2.7276, 2.8789], [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>) \"\"\" This is because the attention layers use the padding tokens in the context they look at for each token in the sentence. Attention layers attend just the 4 tokens: [I, hate, this, !] Attention layers attend the 4 tokens and all padding tokens: [I, hate, this, !, [PAD], [PAD], [PAD], [PAD]] To tell the attention layers to ignore the padding tokens, we need to pass them an attention mask. all_ids = tf . convert_to_tensor ( [[ 1045 , 1005 , 2310 , 2042 , 3403 , 2005 , 1037 , 17662 , 12172 , 2607 , 2026 , 2878 , 2166 , 1012 ], [ 1045 , 5223 , 2023 , 1012 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) # adding attention by creating attention mask attention_mask = tf . convert_to_tensor ( [[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]] ) Here, attention layers will ignore the tokens marked with 0 in the attention mask. With the proper attention mask, predictions are the same for a given sentence, with or without padding. model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) output1 = model ( ids1 ) output2 = model ( ids2 ) print ( output1 . logits ) print ( output2 . logits ) output = model ( all_ids , attention_mask = attention_mask ) print ( output . logits ) Using with padding=True, the tokenizer can directly prepare the inputs with padding and the proper attention mask: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sentences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"I hate this.\" , ] print ( tokenizer ( sentences , padding = True ))","title":"Batching inputs together:"},{"location":"2_5PuttingAllTogether/","text":"We\u2019ve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, attention masks. Tokenization When we call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) # tokenize a single sequence sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) # tokenize multiple sequences at a time sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] model_inputs = tokenizer ( sequences ) It can pad according to several objectives: # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer ( sequences , padding = \"longest\" ) # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , padding = \"max_length\" ) # Will pad the sequences up to the specified max length model_inputs = tokenizer ( sequences , padding = \"max_length\" , max_length = 8 ) It can also truncate sequences: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , truncation = True ) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer ( sequences , max_length = 8 , truncation = True ) The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. Prompting the tokenizer to return tensors from the different frameworks \u2014 \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Returns PyTorch tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"pt\" ) # Returns TensorFlow tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"tf\" ) # Returns NumPy arrays model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"np\" ) Special tokens sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) print ( model_inputs [ \"input_ids\" ]) tokens = tokenizer . tokenize ( sequence ) ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( ids ) # decode the tokens print ( tokenizer . decode ( model_inputs [ \"input_ids\" ])) print ( tokenizer . decode ( ids )) The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don\u2019t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. Wrapping up: From tokenizer to model one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API: import tensorflow as tf from transformers import AutoTokenizer , TFAutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] tokens = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"tf\" ) output = model ( ** tokens )","title":"Putting All Together"},{"location":"2_5PuttingAllTogether/#tokenization","text":"When we call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model: from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) # tokenize a single sequence sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) # tokenize multiple sequences at a time sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] model_inputs = tokenizer ( sequences ) It can pad according to several objectives: # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer ( sequences , padding = \"longest\" ) # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , padding = \"max_length\" ) # Will pad the sequences up to the specified max length model_inputs = tokenizer ( sequences , padding = \"max_length\" , max_length = 8 ) It can also truncate sequences: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer ( sequences , truncation = True ) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer ( sequences , max_length = 8 , truncation = True ) The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. Prompting the tokenizer to return tensors from the different frameworks \u2014 \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays: sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] # Returns PyTorch tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"pt\" ) # Returns TensorFlow tensors model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"tf\" ) # Returns NumPy arrays model_inputs = tokenizer ( sequences , padding = True , return_tensors = \"np\" )","title":"Tokenization"},{"location":"2_5PuttingAllTogether/#special-tokens","text":"sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer ( sequence ) print ( model_inputs [ \"input_ids\" ]) tokens = tokenizer . tokenize ( sequence ) ids = tokenizer . convert_tokens_to_ids ( tokens ) print ( ids ) # decode the tokens print ( tokenizer . decode ( model_inputs [ \"input_ids\" ])) print ( tokenizer . decode ( ids )) The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don\u2019t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end.","title":"Special tokens"},{"location":"2_5PuttingAllTogether/#wrapping-up-from-tokenizer-to-model","text":"one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API: import tensorflow as tf from transformers import AutoTokenizer , TFAutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"So have I!\" ] tokens = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"tf\" ) output = model ( ** tokens )","title":"Wrapping up: From tokenizer to model"},{"location":"3FineTuningPretrainedModel/","text":"Introduction We will learn: How to prepare a large dataset from the Hub How to use Keras to fine-tune a model How to use Keras to get predictions How to use a custom metric","title":"Introduction"},{"location":"3FineTuningPretrainedModel/#introduction","text":"We will learn: How to prepare a large dataset from the Hub How to use Keras to fine-tune a model How to use Keras to get predictions How to use a custom metric","title":"Introduction"},{"location":"3_1ProcessingData/","text":"Processing the data Train a sequence classifier on one batch in TensorFlow: import tensorflow as tf import numpy as np from transformers import AutoTokenizer , TFAutoModelForSequenceClassification # Same as before checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = dict ( tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"tf\" )) # This is new model . compile ( optimizer = \"adam\" , loss = \"sparse_categorical_crossentropy\" ) labels = tf . convert_to_tensor ([ 1 , 1 ]) model . train_on_batch ( batch , labels ) - Better accuracy required bigger dataset. - Dataset used: MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper - The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). Loading a dataset from the Hub Huggingface dataset library is a library that provides an api to quickly download many public's datasets and preprocess them. Huggingface dataset library allows you to easily download and cache datasets from its identifier on dataset hub. MRPC dataset is one of the 10 datasets composing the GLUE benchmark , which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). from datasets import load_dataset raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) raw_datasets It returns a DatasetDict object which is a sort of dictionary containing each split(train,validation & test set) of the dataset. We can access any split of dataset by its key, then any element by index. raw_datasets [ \"train\" ] Each of the splits contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set). We can access en element(s) by indexing: raw_datasets [ \"train\" ][ 6 ] raw_datasets [ \"train\" ][: 5 ] We can also directly get a slice of your dataset. The features attributes gives us more information about each column. It gives corresponds between integer and names for tha labels. raw_datasets [ \"train\" ] . features The map method allows you to apply a function over all the splits of a given dataset. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( example ): return tokenizer ( example [ \"sentence1\" ], example [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function ) print ( tokenized_datasets . column_names ) As long as the function returns a dictionary like object, the map() method will add new columns as needed or update existing ones. Result of tokenize_function(): ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'] You can preprocess faster by using the option batched=True. The applied function will then receive multiple examples at each call. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) We can aslo use multiprocessing with a map method. With just a few last tweaks, the dataset is then ready for training! We just remove the columns we don't need anymore with the remove column methods, rename label to labels since the model from transformers library expect that and set the output format to desired backend torch, tensorflow or numpy. If needed we can also generate a short sample of dataset using the select method. tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"tensorflow\" ) tokenized_datasets [ \"train\" ] small_train_dataset = tokenized_datasets [ \"train\" ] . select ( range ( 100 )) Preprocessing a dataset Preprocessing sentence pairs (TensorFlow) We have seen before how to tokenize single sentences and batch them together. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"tf\" ) But text classification can also be applied on pairs of sentences. In a problem called Natural Language Inference(NLI), were we check whether a pair of sentences are logically related or not. In fact, the GLUE benchmark, 8 of the 10 tasks concern pair of sentences. Datasets with single sentences: COLA, SST-2 Datasets with pairs of sentences: MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI Models like BERT are pretrained to recognize relationships between 2 sentences. The tokenizers accept sentence pairs as well as single sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( \"My name is Pallavi Saxena.\" , \"I work at Avalara.\" ) It returns a new field called \"token_type_ids\" which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. The tokenizer adds special tokens for the corresponding model and prepares \"token_type_ids\" to indicate which part of the inputs correspond to which sentence. To process several pairs of sentences together, just pass the list of first sentences followed by the list of second sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True ) Tokenizers prepare the proper token type IDs and attention masks. Those inputs are then ready to go through a sequence classification model! from transformers import TFAutoModelForSequenceClassification , AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) batch = tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True , return_tensors = \"tf\" , ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( ** batch ) All model checkpoint layers were used when initializing TFBertForSequenceClassification. Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Dynamic padding We need to pad sentences of different lengths to make batches. The first way to do this is to pad all the sentences in the whole datset to the maximum length in the dataset. Its downside is sentences with short sentences will have a lot of padding tokens, which will include more computations in the model which actually not needed. Another way is to pad the sentences at the batch creation, to the length of the longest sentence; a technique called dynamic padding. Pros: All the batches will have the smallest size possible. Con: dynamic shapes don't work well on the accelerators. All batches have different shapes slows down things on accelerators like TPUs. In practice, here is how we can preprocess the MRPC dataset with fixed padding: from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) We can then use our dataset in a standard PyTorch DataLoader. As expected, we get batches of fixed shapes from torch.utils.data import DataLoader train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break To apply dynamic padding, we postpone the padding in the preprocessing function. from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) Each batch then has a different size, but there is no needless padding. from torch.utils.data import DataLoader from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding ( tokenizer ) train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True , collate_fn = data_collator ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break","title":"Processing Data"},{"location":"3_1ProcessingData/#processing-the-data","text":"Train a sequence classifier on one batch in TensorFlow: import tensorflow as tf import numpy as np from transformers import AutoTokenizer , TFAutoModelForSequenceClassification # Same as before checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = dict ( tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"tf\" )) # This is new model . compile ( optimizer = \"adam\" , loss = \"sparse_categorical_crossentropy\" ) labels = tf . convert_to_tensor ([ 1 , 1 ]) model . train_on_batch ( batch , labels ) - Better accuracy required bigger dataset. - Dataset used: MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper - The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).","title":"Processing the data"},{"location":"3_1ProcessingData/#loading-a-dataset-from-the-hub","text":"Huggingface dataset library is a library that provides an api to quickly download many public's datasets and preprocess them. Huggingface dataset library allows you to easily download and cache datasets from its identifier on dataset hub. MRPC dataset is one of the 10 datasets composing the GLUE benchmark , which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in paper The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). from datasets import load_dataset raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) raw_datasets It returns a DatasetDict object which is a sort of dictionary containing each split(train,validation & test set) of the dataset. We can access any split of dataset by its key, then any element by index. raw_datasets [ \"train\" ] Each of the splits contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set). We can access en element(s) by indexing: raw_datasets [ \"train\" ][ 6 ] raw_datasets [ \"train\" ][: 5 ] We can also directly get a slice of your dataset. The features attributes gives us more information about each column. It gives corresponds between integer and names for tha labels. raw_datasets [ \"train\" ] . features The map method allows you to apply a function over all the splits of a given dataset. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( example ): return tokenizer ( example [ \"sentence1\" ], example [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function ) print ( tokenized_datasets . column_names ) As long as the function returns a dictionary like object, the map() method will add new columns as needed or update existing ones. Result of tokenize_function(): ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'] You can preprocess faster by using the option batched=True. The applied function will then receive multiple examples at each call. from transformers import AutoTokenizer checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) We can aslo use multiprocessing with a map method. With just a few last tweaks, the dataset is then ready for training! We just remove the columns we don't need anymore with the remove column methods, rename label to labels since the model from transformers library expect that and set the output format to desired backend torch, tensorflow or numpy. If needed we can also generate a short sample of dataset using the select method. tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"tensorflow\" ) tokenized_datasets [ \"train\" ] small_train_dataset = tokenized_datasets [ \"train\" ] . select ( range ( 100 ))","title":"Loading a dataset from the Hub"},{"location":"3_1ProcessingData/#preprocessing-a-dataset","text":"","title":"Preprocessing a dataset"},{"location":"3_1ProcessingData/#preprocessing-sentence-pairs-tensorflow","text":"We have seen before how to tokenize single sentences and batch them together. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\" , \"This course is amazing!\" , ] batch = tokenizer ( sequences , padding = True , truncation = True , return_tensors = \"tf\" ) But text classification can also be applied on pairs of sentences. In a problem called Natural Language Inference(NLI), were we check whether a pair of sentences are logically related or not. In fact, the GLUE benchmark, 8 of the 10 tasks concern pair of sentences. Datasets with single sentences: COLA, SST-2 Datasets with pairs of sentences: MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI Models like BERT are pretrained to recognize relationships between 2 sentences. The tokenizers accept sentence pairs as well as single sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( \"My name is Pallavi Saxena.\" , \"I work at Avalara.\" ) It returns a new field called \"token_type_ids\" which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. The tokenizer adds special tokens for the corresponding model and prepares \"token_type_ids\" to indicate which part of the inputs correspond to which sentence. To process several pairs of sentences together, just pass the list of first sentences followed by the list of second sentences. from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True ) Tokenizers prepare the proper token type IDs and attention masks. Those inputs are then ready to go through a sequence classification model! from transformers import TFAutoModelForSequenceClassification , AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) batch = tokenizer ( [ \"My name is Pallavi Saxena.\" , \"Going to the cinema.\" ], [ \"I work at Avalara.\" , \"This movie is great.\" ], padding = True , return_tensors = \"tf\" , ) model = TFAutoModelForSequenceClassification . from_pretrained ( checkpoint ) outputs = model ( ** batch ) All model checkpoint layers were used when initializing TFBertForSequenceClassification. Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.","title":"Preprocessing sentence pairs (TensorFlow)"},{"location":"3_1ProcessingData/#dynamic-padding","text":"We need to pad sentences of different lengths to make batches. The first way to do this is to pad all the sentences in the whole datset to the maximum length in the dataset. Its downside is sentences with short sentences will have a lot of padding tokens, which will include more computations in the model which actually not needed. Another way is to pad the sentences at the batch creation, to the length of the longest sentence; a technique called dynamic padding. Pros: All the batches will have the smallest size possible. Con: dynamic shapes don't work well on the accelerators. All batches have different shapes slows down things on accelerators like TPUs. In practice, here is how we can preprocess the MRPC dataset with fixed padding: from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], padding = \"max_length\" , truncation = True , max_length = 128 ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) We can then use our dataset in a standard PyTorch DataLoader. As expected, we get batches of fixed shapes from torch.utils.data import DataLoader train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break To apply dynamic padding, we postpone the padding in the preprocessing function. from datasets import load_dataset from transformers import AutoTokenizer raw_datasets = load_dataset ( \"glue\" , \"mrpc\" ) checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer . from_pretrained ( checkpoint ) def tokenize_function ( examples ): return tokenizer ( examples [ \"sentence1\" ], examples [ \"sentence2\" ], truncation = True ) tokenized_datasets = raw_datasets . map ( tokenize_function , batched = True ) tokenized_datasets = tokenized_datasets . remove_columns ([ \"idx\" , \"sentence1\" , \"sentence2\" ]) tokenized_datasets = tokenized_datasets . rename_column ( \"label\" , \"labels\" ) tokenized_datasets = tokenized_datasets . with_format ( \"torch\" ) Each batch then has a different size, but there is no needless padding. from torch.utils.data import DataLoader from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding ( tokenizer ) train_dataloader = DataLoader ( tokenized_datasets [ \"train\" ], batch_size = 16 , shuffle = True , collate_fn = data_collator ) for step , batch in enumerate ( train_dataloader ): print ( batch [ \"input_ids\" ] . shape ) if step > 5 : break","title":"Dynamic padding"},{"location":"3_2ModelFineTuningWithKeras/","text":"","title":"Fine-Tuning A Model with Keras"}]}