{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW2AHYoNZ68z"
      },
      "source": [
        "# Handling multiple sequences (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL9O2VjytaFP"
      },
      "source": [
        "## Models expect a batch of inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw-PDMUVZ687",
        "outputId": "9618644a-14dd-4023-bc25-c81788f35bce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8VGSR3QZ688",
        "outputId": "d2503b29-212a-4145-f8b0-ec9445132ccf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
              "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
              "        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9oYBRHjZ689",
        "outputId": "8763899f-07c4-40a5-8474-657db10675d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Input IDs: tf.Tensor(\n",
              "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
              "   2166  1012]], shape=(1, 14), dtype=int32)\n",
              "Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = tf.constant([ids]) # A new dimension added here\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0Kyl2ZtoRF"
      },
      "outputs": [],
      "source": [
        "batched_ids = [ids, ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHCvQVQ9uGtV"
      },
      "source": [
        "## Padding the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INPH7jRnZ68-"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfoglaWZ68_"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2NOXXO1Z69A",
        "outputId": "8e640662-8c5d-47d7-a20e-774b1eb76769"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)\n",
              "tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)\n",
              "tf.Tensor(\n",
              "[[ 1.5693681 -1.3894582]\n",
              " [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(tf.constant(sequence1_ids)).logits)\n",
        "print(model(tf.constant(sequence2_ids)).logits)\n",
        "print(model(tf.constant(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YMZqtIiuVAm"
      },
      "source": [
        "## Attention masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiOhIwXkZ69B",
        "outputId": "41c2fa0d-6143-48f4-b02b-8073d86ce227"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tf.Tensor(\n",
              "[[ 1.5693681  -1.3894582 ]\n",
              " [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgA1cksucOM"
      },
      "source": [
        "## Longer sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHp7Db1eZ69C"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcaIaiqWue0m"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLHPG4M_u5ss"
      },
      "source": [
        "## Batching inputs together "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNlIWNZRuzkp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\",\n",
        "]\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "print(ids[0])\n",
        "print(ids[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLFckh9C5MQA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "       [1045, 5223, 2023, 1012]]\n",
        "\n",
        "input_ids = tf.constant(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMZJ8JwW6cMJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "       [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        "\n",
        "input_ids = tf.constant(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-u02696cuA"
      },
      "outputs": [],
      "source": [
        "### adding padding\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNbtKAfq7M6k"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "ids1 = tf.constant(\n",
        "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]]\n",
        ")\n",
        "ids2 = tf.constant([[1045, 5223, 2023, 1012]])\n",
        "all_ids = tf.constant(\n",
        "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "     [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        ")\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "print(model(ids1).logits)\n",
        "print(model(ids2).logits)\n",
        "print(model(all_ids).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxWgrh4t6c3i"
      },
      "outputs": [],
      "source": [
        "# adding attention by creating attention mask\n",
        "all_ids = tf.constant(\n",
        "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "     [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        ")\n",
        "attention_mask = tf.constant(\n",
        "    [[   1,    1,    1,    1,    1,    1,    1,     1,     1,    1,    1,    1,    1,    1],\n",
        "     [   1,    1,    1,    1,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeQ1UkwQ7yNb"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "output1 = model(ids1)\n",
        "output2 = model(ids2)\n",
        "print(output1.logits)\n",
        "print(output2.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjhsNWCw70HE"
      },
      "outputs": [],
      "source": [
        "output = model(all_ids, attention_mask=attention_mask)\n",
        "print(output.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO_JXLpo8ROe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\",\n",
        "]\n",
        "print(tokenizer(sentences, padding=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS-L59_G8UoO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1982fbf211ad5dee3df527f8dde59caaf96cdcf1132c131fb0bff5660ddf9aa4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
